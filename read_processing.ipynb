{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  var JS_MIME_TYPE = 'application/javascript';\n  var HTML_MIME_TYPE = 'text/html';\n  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  var CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    var script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    var cell = handle.cell;\n\n    var id = cell.output_area._bokeh_element_id;\n    var server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd, {\n        iopub: {\n          output: function(msg) {\n            var id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    var output_area = handle.output_area;\n    var output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      var bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      var script_attrs = bk_div.children[0].attributes;\n      for (var i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      var toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\": \"YF85VygJKMVnHE+lLv2AM93Vbstr0yo2TbIu5v8se5Rq3UQAUmcuh4aaJwNlpKwa\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\": \"KKuas3gevv3PvrlkyCMzffFeaMq5we/a2QsP5AUoS3mJ0jmaCL7jirFJN3GoE/lM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\": \"MK/uFc3YT18pkvvXRl66tTHjP0/dxoSH2e/eiNMFIguKlun2+WVqaPTWmUy/zvh4\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from subprocess import Popen\n",
    "from time import sleep, time\n",
    "import signal\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, output_notebook, push_notebook\n",
    "from bokeh.models import ColumnDataSource, CDSView\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import brewer\n",
    "from collections import OrderedDict\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genomes generated with:\n",
    "```\n",
    "STAR --runThreadN 30 --runMode genomeGenerate --genomeDir /home/anthony/minreporter_profiling/star_indices/minopt --genomeFastaFiles /home/anthony/minreporter_profiling/annotations_from_colin/S_cer_chr_OPT.fa --sjdbGTFfile /home/anthony/minreporter_profiling/annotations_from_colin/reporter_genome_annos_with_UTRS/OPT.gtf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set paths/parameters\n",
    "fastq_dir = \"/home/anthony/minreporter_profiling/fastq/round3/profiling/minopt/\"\n",
    "output_dir = \"/home/anthony/minreporter_profiling/output/round3/pipeline_output_minopt/\"\n",
    "available_cores = 25 #TODO:deal with situation if threads are fewer than number of samples.\n",
    "star_ncRNA_dir = \"/home/anthony/minreporter_profiling/annotations_from_colin/noncoding_star_scer3/\"\n",
    "star_genome_dir = \"/home/anthony/minreporter_profiling/star_indices/minopt/\"\n",
    "star_reporter_dir = False # if no reporter, set to False\n",
    "\n",
    "fastq_dir += \"/\" if not fastq_dir.endswith(\"/\") else \"\"\n",
    "output_dir += \"/\" if not output_dir.endswith(\"/\") else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [], 'B': [], 'C': [], 'D': [], 'E': [], 'F': [], 'G': [], 'H': [], 'I': [], 'J': [], 'K': [], 'L': [], 'M': []}\n"
     ]
    }
   ],
   "source": [
    "#rename fasta files\n",
    "barcode_name_mapping = {\n",
    "    'A': 'wt_0_mono_r2',\n",
    "    'B': 'wt_100_mono_r2',\n",
    "    'C': 'wt_cga_mono_r2',\n",
    "    'D': 'hel2_cga_mono_r1',\n",
    "    'E': 'hel2_cga_mono_r2',\n",
    "    'F': 'wt_0_di_r1',\n",
    "    'G': 'wt_0_di_r2',\n",
    "    'H': 'wt_100_di_r1',\n",
    "    'I': 'wt_100_di_r2',\n",
    "    'J': 'wt_cga_di_r1',\n",
    "    'K': 'wt_cga_di_r2',\n",
    "    'L': 'hel2_cga_di_r1',\n",
    "    'M': 'hel2_cga_di_r2',\n",
    "}\n",
    "\n",
    "files_dict = {}\n",
    "for key in \"ABCDEFGHIJKLM\":\n",
    "    files_dict[key] = []\n",
    "\n",
    "files = os.listdir(fastq_dir)\n",
    "for file in files:\n",
    "    match = re.search(\"(.+)(\\.fastq|\\.fastq\\.gz)$\", file)\n",
    "    if match:\n",
    "        files_dict[file[0]].append(file)\n",
    "\n",
    "print(files_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up folder structure for output\n",
    "if os.access(output_dir, os.F_OK):\n",
    "    if not os.access(output_dir+\"output\", os.F_OK):\n",
    "        os.chdir(output_dir)\n",
    "        os.mkdir(\"output\")\n",
    "        os.mkdir(output_dir+\"/output/logs\")\n",
    "        os.mkdir(output_dir+\"/output/logs/pipeline_completion\")\n",
    "        os.mkdir(output_dir+\"/output/deduplicated\")\n",
    "        os.mkdir(output_dir+\"/output/trimmed\")\n",
    "        os.mkdir(output_dir+\"/output/ncRNA_aligned\")\n",
    "        if star_reporter_dir:\n",
    "            os.mkdir(output_dir+\"/output/reporter_aligned\")\n",
    "        os.mkdir(output_dir+\"/output/genome_aligned\")\n",
    "    output_dir += \"output/\"\n",
    "else:\n",
    "    print(\"WARNING:\", output_dir, \"does not exist.\")\n",
    "if not os.access(fastq_dir, os.F_OK):\n",
    "    print(\"WARNING:\", fastq_dir, \"does not exist.\")\n",
    "\n",
    "#check file endings for correct data processing.\n",
    "filenames = os.listdir(fastq_dir)\n",
    "fastq_files = []\n",
    "for filename in filenames:\n",
    "    if filename.endswith(\".fastq\") or filename.endswith(\".fastq.gz\"):\n",
    "        fastq_files.append(filename)\n",
    "\n",
    "#calculate the number of cores by dividing available cores by number of samples.\n",
    "numcores = max(available_cores//len(fastq_files), 1)\n",
    "\n",
    "print(\"Ready to process files:\")\n",
    "[print(\"\\t\\t\"+file) for file in fastq_files]\n",
    "print(\"\\nUsing\", str(numcores)+\"/\"+str(available_cores), \"cores for each sample.\")\n",
    "print(\"Data will be output to\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines process handler for running one sample through the pipeline\n",
    "class run_one_sample():\n",
    "    '''handles the running of one sample through the entire pipeline. \n",
    "    This class should be called by an individual thread. It will output stdout/stderr \n",
    "    readouts from the individual steps in the .../output/logs/ folder.\n",
    "    \n",
    "    It also uses a simple text log to pick up where the sample left off if the pipeline \n",
    "    fails. This log is stored in .../output/logs/pipeline_completion/\n",
    "    Steps are defined in self.steps\n",
    "    \n",
    "    The self.check method will terminate the run and kill all running process groups if\n",
    "    one of the processes in this thread returns an exit code other than 0, or if any\n",
    "    other thread changes the global kill_pipeline variable to True. Default behavior is \n",
    "    to set kill_pipeline to True whenever any thread fails, thus halting processing of \n",
    "    all samples.\n",
    "    \n",
    "    On completion, each thread will add one to the global samples_done counter, to \n",
    "    let the main thread know when all are finished.\n",
    "    \n",
    "    TODO:use queues/messages instead of changing global variables\n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        global kill_pipeline\n",
    "        global samples_done\n",
    "        global star_ncRNA_dir\n",
    "        global star_genome_dir\n",
    "        global star_reporter_dir\n",
    "        global fastq_dir\n",
    "        global output_dir\n",
    "        global numcores\n",
    "        global failed_sample\n",
    "        global thread_tracker\n",
    "        self.star_ncRNA_dir = star_ncRNA_dir\n",
    "        self.star_genome_dir = star_genome_dir\n",
    "        self.star_reporter_dir = star_reporter_dir\n",
    "        self.fastq_dir = fastq_dir\n",
    "        self.filename = filename\n",
    "        self.output_dir = output_dir\n",
    "        self.numcores = numcores\n",
    "        self.killflag = False\n",
    "        match = re.search(\"(.+)(\\.fastq|\\.fastq\\.gz)$\", self.filename)\n",
    "        self.prefix = match.group(1)\n",
    "        self.suffix = match.group(2)\n",
    "        if self.prefix not in thread_tracker:\n",
    "            thread_tracker[self.prefix] = \"initiate\"\n",
    "        self.steps = self.define_steps()\n",
    "        \n",
    "        with open(output_dir+\"/logs/\"+self.prefix+\"_stdout.txt\", \"a\") as self.stdout, \\\n",
    "            open(output_dir+\"/logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", \"a\") as self.completion:\n",
    "            try:\n",
    "                for step_string in self.steps.keys():\n",
    "                    if self.killflag == False and kill_pipeline == False:\n",
    "                        function_name, function_args = self.steps[step_string]\n",
    "                        self.process = function_name(*function_args)\n",
    "                        self.check(self.process)\n",
    "                        self.completion.write(step_string+\"\\n\")\n",
    "                        self.completion.flush()\n",
    "                        thread_tracker[self.prefix] = step_string\n",
    "                samples_done += 1\n",
    "                        \n",
    "            except:\n",
    "                self.killflag = True\n",
    "                kill_pipeline = True\n",
    "                samples_done += 1 \n",
    "                raise\n",
    "                \n",
    "    \n",
    "    def define_steps(self):\n",
    "        steps = OrderedDict([\n",
    "            \n",
    "            (\"deduplicate\", (self.deduplicate, (fastq_dir, output_dir, self.filename, numcores))),\n",
    "            (\"trim\", (self.trim_reads, (output_dir, self.filename, numcores))),\n",
    "            (\"align_ncRNA\", (self.align_ncRNA, (output_dir, self.filename, numcores))),\n",
    "            (\"align_genome\", (self.align_genome, (output_dir, self.filename, numcores))),\n",
    "            (\"sort_genome_aligned\", (self.samtools_sort, (\"genome_aligned\", numcores))),\n",
    "            (\"index_genome_aligned\", (self.samtools_index, (\"genome_aligned\", numcores))),\n",
    "        ])\n",
    "            \n",
    "        if os.access(output_dir+\"logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", os.F_OK):\n",
    "            with open(output_dir+\"logs/pipeline_completion/\"+self.prefix+\"_completion_log.txt\", \"r\") as self.completion:\n",
    "                for line in self.completion.readlines():\n",
    "                    steps.pop(line.strip())\n",
    "        return steps\n",
    "    \n",
    "    \n",
    "    def deduplicate(self, fastq_dir, output_dir, filename, numcores):\n",
    "        '''deduplicate ribosome profiling reads using dedupe.sh from \n",
    "        the BBTools suite.\n",
    "        '''\n",
    "        self.stdout.write(\"Deduplicate\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        dedupe_dir = output_dir+\"deduplicated/\"+self.prefix+\"/\"\n",
    "        if not os.access(dedupe_dir, os.F_OK):\n",
    "            os.mkdir(dedupe_dir)\n",
    "        return Popen([\n",
    "            \"dedupe.sh\",\n",
    "            \"in=\"+fastq_dir+filename,\n",
    "            \"out=\"+dedupe_dir+filename,\n",
    "            \"absorbmatch=t\", #absorb exact matches of contigs\n",
    "            \"absorbcontainment=f\", #do not absorb full containments of contigs\n",
    "            \"absorbrc=f\", #do not absorb reverse-compliments\n",
    "            \"threads=\"+str(numcores),\n",
    "            \"overwrite=t\",\n",
    "            \"ignorebadquality=t\",\n",
    "        ], stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def trim_reads(self, output_dir, filename, numcores):\n",
    "        '''Trim adapters and low quality regions from reads using bbduk.sh\n",
    "        from the BBTools suite.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nTrim Reads\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "        if not os.access(trimmed_dir, os.F_OK):\n",
    "            os.mkdir(trimmed_dir)\n",
    "            os.mkdir(trimmed_dir+\"failedQC\")\n",
    "        return Popen([\n",
    "            \"bbduk.sh\",\n",
    "            \"in=\"+output_dir+\"deduplicated/\"+self.prefix+\"/\"+filename,\n",
    "            \"out=\"+trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "            \"outm=\"+trimmed_dir+\"failedQC/\"+self.prefix+\".failedQC\"+self.suffix,\n",
    "            \"rpkm=\"+trimmed_dir+\"rpkm.txt\",\n",
    "            \"refstats=\"+trimmed_dir+\"trimming_stats.txt\",\n",
    "            \"literal=NNNNNNCACTCGGGCACCAAGGAC\",\n",
    "            \"k=24\", # this parameter sets the minimum kmer being trimmed. \n",
    "                                  #Longer = more specific, shorter = more sensitive\n",
    "            \"mink=8\", #includes truncations of the kmers down to 8\n",
    "            \"mm=f\", #do not ignore middle base mismatch of kmer\n",
    "            \"rcomp=f\", #do not allow reverse complement kmer matches\n",
    "            \"copyundefined=t\",\n",
    "            \"ktrim=r\",\n",
    "            \"forcetrimleft=4\", #removes random barcode on left of reads.\n",
    "            \"minavgquality=10\",\n",
    "            \"minlength=10\",\n",
    "            \"threads=\"+str(numcores),\n",
    "            \"overwrite=t\",\n",
    "        ],\n",
    "        stdout=self.stdout, stderr=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "\n",
    "    def align_ncRNA(self, output_dir, filename, numcores):\n",
    "        '''Align reads to ncRNA using STAR. ncRNA fasta sequences from Boris.\n",
    "        Output unaligned reads.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nAlign to ncRNA\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        trimmed_dir = output_dir+\"trimmed/\"+self.prefix+\"/\"\n",
    "        ncRNA_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "        if not os.access(ncRNA_aligned_dir, os.F_OK):\n",
    "            os.mkdir(ncRNA_aligned_dir)\n",
    "        command = [\n",
    "            \"STAR\",\n",
    "            \"--runThreadN\", str(numcores),\n",
    "            \"--genomeDir\", self.star_ncRNA_dir,\n",
    "            \"--readFilesIn\", trimmed_dir+self.prefix+\".trimmed.fastq\",\n",
    "            \"--outFileNamePrefix\", ncRNA_aligned_dir+self.prefix+\"_\",\n",
    "            \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "            \"--outReadsUnmapped\", \"Fastx\",\n",
    "            \"--alignSJDBoverhangMin\", \"1\",\n",
    "            \"--alignSJoverhangMin\", \"8\",\n",
    "            \"--outFilterMultimapNmax\", \"20\",\n",
    "            \"--outFilterType\", \"BySJout\",\n",
    "        ]\n",
    "        return Popen(command, stderr=self.stdout, stdout=self.stdout, \n",
    "                     preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def align_reporter(self, output_dir, filename, numcores):\n",
    "        '''Align reads to reporter sequence using STAR.\n",
    "        Output unaligned reads.\n",
    "        '''\n",
    "        if self.star_reporter_dir:\n",
    "            self.stdout.write(\"\\n\\nAlign to reporter\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            ncRNA_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "            reporter_aligned_dir = output_dir+\"reporter_aligned/\"+self.prefix+\"/\"\n",
    "            if not os.access(reporter_aligned_dir, os.F_OK):\n",
    "                os.mkdir(reporter_aligned_dir)\n",
    "            command = [\n",
    "                \"STAR\",\n",
    "                \"--runThreadN\", str(numcores),\n",
    "                \"--genomeDir\", self.star_reporter_dir,\n",
    "                \"--readFilesIn\", ncRNA_aligned_dir+self.prefix+\"_Unmapped.out.mate1\",\n",
    "                \"--outFileNamePrefix\", reporter_aligned_dir+self.prefix+\"_\",\n",
    "                \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "                \"--outReadsUnmapped\", \"Fastx\",\n",
    "                \"--alignSJDBoverhangMin\", \"1\",\n",
    "                \"--alignSJoverhangMin\", \"8\",\n",
    "                \"--outFilterMultimapNmax\", \"1\",\n",
    "                \"--outFilterType\", \"BySJout\",\n",
    "            ]\n",
    "            return Popen(command, stderr=self.stdout, stdout=self.stdout, \n",
    "                         preexec_fn=os.setsid)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def align_genome(self, output_dir, filename, numcores):\n",
    "        '''Align remaining reads to genome.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nAlign to genome\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        if self.star_reporter_dir:\n",
    "            previous_aligned_dir = output_dir+\"reporter_aligned/\"+self.prefix+\"/\"\n",
    "        else:\n",
    "            previous_aligned_dir = output_dir+\"ncRNA_aligned/\"+self.prefix+\"/\"\n",
    "        tx_aligned_dir = output_dir+\"genome_aligned/\"+self.prefix+\"/\"\n",
    "        if not os.access(tx_aligned_dir, os.F_OK):\n",
    "            os.mkdir(tx_aligned_dir)\n",
    "        command = [\n",
    "            \"STAR\",\n",
    "            \"--runThreadN\", str(self.numcores),\n",
    "            \"--genomeDir\", self.star_genome_dir,\n",
    "            \"--readFilesIn\", previous_aligned_dir+self.prefix+\"_Unmapped.out.mate1\",\n",
    "            \"--outFileNamePrefix\", tx_aligned_dir+self.prefix+\"_\",\n",
    "            \"--outSAMtype\", \"BAM\", \"Unsorted\",\n",
    "            \"--outReadsUnmapped\", \"Fastx\",\n",
    "            \"--alignSJDBoverhangMin\", \"1\",\n",
    "            \"--alignSJoverhangMin\", \"8\",\n",
    "            \"--outFilterMismatchNmax\", \"1\",\n",
    "            \"--outFilterMultimapNmax\", \"1\", #how many multimap sites allowed for read\n",
    "            \"--outSAMmultNmax\", \"1\", #how many map sites to write to output for each read\n",
    "            \"--outMultimapperOrder\", \"Random\", #assign read to random alignment if multimapper\n",
    "            \"--outFilterType\", \"BySJout\",\n",
    "        ]\n",
    "        return Popen(command, stderr=self.stdout, stdout=self.stdout,\n",
    "                     preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def samtools_sort(self, input_dir, numcores):\n",
    "        '''Sort BAM file from STAR ouput.\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nSort \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        if input_dir == \"reporter_aligned\":\n",
    "            aligned_suffix = \"_Aligned.out.plusstrand\"\n",
    "        else:\n",
    "            aligned_suffix = \"_Aligned.out\"\n",
    "        return Popen([\n",
    "            \"samtools\",\n",
    "            \"sort\",\n",
    "            \"-@\", str(numcores),\n",
    "            self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",\n",
    "            \"-o\", self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "        ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "    \n",
    "    def samtools_index(self, input_dir, numcores):\n",
    "        '''Index BAM file from STAR output\n",
    "        '''\n",
    "        self.stdout.write(\"\\n\\nIndex \"+input_dir+\" BAM file\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "        self.stdout.flush()\n",
    "        if input_dir == \"reporter_aligned\":\n",
    "            aligned_suffix = \"_Aligned.out.plusstrand\"\n",
    "        else:\n",
    "            aligned_suffix = \"_Aligned.out\"\n",
    "        try:\n",
    "            os.remove(self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".bam\",)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        return Popen([\n",
    "            \"samtools\",\n",
    "            \"index\",\n",
    "            \"-@\", str(numcores),\n",
    "            self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+aligned_suffix+\".sorted.bam\"\n",
    "        ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "            \n",
    "        \n",
    "    def samtools_remove_rv_reads(self, input_dir, numcores):\n",
    "        '''Remove reverse reads aligned to the reporter minus strand with samtools\n",
    "        '''\n",
    "        if self.star_reporter_dir:\n",
    "            self.stdout.write(\"\\n\\nRemove reporter reverse strand reads\\n\"+\"\".join([\"-\"]*20)+\"\\n\\n\")\n",
    "            self.stdout.flush()\n",
    "            return Popen([\n",
    "                \"samtools\",\n",
    "                \"view\",\n",
    "                \"-@\", str(numcores),\n",
    "                \"-F\", \"0x10\", #only include reads with this flag, which means plus strand alignment\n",
    "                \"-o\",\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+\"_Aligned.out.plusstrand.bam\",\n",
    "                self.output_dir+input_dir+\"/\"+self.prefix+\"/\"+self.prefix+\"_Aligned.out.bam\"\n",
    "            ], stderr=self.stdout, stdout=self.stdout, preexec_fn=os.setsid)\n",
    "                \n",
    "            \n",
    "        \n",
    "    def check(self, proc):\n",
    "        '''Poll Popen processes returned by each method to determine if a nonzero\n",
    "        error code was returned. If so, kill process group and set global kill_pipeline\n",
    "        to True, signalling to other processing threads to shut down as well.\n",
    "        Polling happens every 1 second.\n",
    "        '''\n",
    "        global kill_pipeline\n",
    "        global thread_tracker\n",
    "        exit_code = proc.poll()\n",
    "        while exit_code == None:\n",
    "            sleep(1)\n",
    "            if kill_pipeline == True:\n",
    "                self.killflag = True\n",
    "            if self.killflag == True:\n",
    "                os.killpg(os.getpgid(proc.pid), signal.SIGTERM)\n",
    "            exit_code = proc.poll()\n",
    "        else:\n",
    "            if exit_code != 0:\n",
    "                self.killflag = True\n",
    "                kill_pipeline = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Running...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"1a434666-084d-4a6d-ba98-390efe12d259\" data-root-id=\"8183\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n    \n  var docs_json = {\"ade1220c-643f-4b2e-ae03-2511d877dd06\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"background_fill_color\":\"lightgray\",\"below\":[{\"id\":\"8194\"}],\"center\":[{\"id\":\"8196\"},{\"id\":\"8199\"}],\"height\":200,\"left\":[{\"id\":\"8197\"}],\"renderers\":[{\"id\":\"8206\"}],\"title\":{\"id\":\"8184\"},\"toolbar\":{\"id\":\"8200\"},\"width\":800,\"x_range\":{\"id\":\"8186\"},\"x_scale\":{\"id\":\"8190\"},\"y_range\":{\"id\":\"8188\"},\"y_scale\":{\"id\":\"8192\"}},\"id\":\"8183\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"text\":\"Pipeline Progress\"},\"id\":\"8184\",\"type\":\"Title\"},{\"attributes\":{\"active_multi\":null},\"id\":\"8200\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"8242\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis\":{\"id\":\"8197\"},\"dimension\":1,\"ticker\":null,\"visible\":false},\"id\":\"8199\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"8198\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"8243\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"8192\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"formatter\":{\"id\":\"8239\"},\"major_label_policy\":{\"id\":\"8240\"},\"ticker\":{\"id\":\"8198\"}},\"id\":\"8197\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"8201\"},\"glyph\":{\"id\":\"8204\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"8205\"},\"view\":{\"id\":\"8207\"}},\"id\":\"8206\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"8190\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"factors\":[\"Start\",\"Deduplicating\",\"Trimming Adapter\",\"Aligning to ncRNA\",\"Aligning to Genome\",\"Sorting reporter aligned reads\",\"Indexing Genome Aligned Reads\",\"Done\"]},\"id\":\"8186\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"8195\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"factors\":[\"wt_100_di_r1\",\"wt_100_di_r2\",\"wt_100_mono_r1\",\"wt_100_mono_r2\"]},\"id\":\"8188\",\"type\":\"FactorRange\"},{\"attributes\":{\"source\":{\"id\":\"8201\"}},\"id\":\"8207\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"8236\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"8237\",\"type\":\"AllLabels\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"y\",\"transform\":{\"id\":\"8202\"}},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"8205\",\"type\":\"HBar\"},{\"attributes\":{\"data\":{\"height\":[0.5,0.5,0.5,0.5],\"left\":[0,0,0,0],\"right\":[\"Start\",\"Start\",\"Start\",\"Start\"],\"y\":[\"wt_100_di_r1\",\"wt_100_di_r2\",\"wt_100_mono_r1\",\"wt_100_mono_r2\"]},\"selected\":{\"id\":\"8243\"},\"selection_policy\":{\"id\":\"8242\"}},\"id\":\"8201\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"8240\",\"type\":\"AllLabels\"},{\"attributes\":{\"formatter\":{\"id\":\"8236\"},\"major_label_orientation\":0.39269908169872414,\"major_label_policy\":{\"id\":\"8237\"},\"ticker\":{\"id\":\"8195\"}},\"id\":\"8194\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"fill_color\":{\"field\":\"y\",\"transform\":{\"id\":\"8202\"}},\"height\":{\"field\":\"height\"},\"left\":{\"field\":\"left\"},\"right\":{\"field\":\"right\"},\"y\":{\"field\":\"y\"}},\"id\":\"8204\",\"type\":\"HBar\"},{\"attributes\":{},\"id\":\"8239\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"8194\"},\"grid_line_color\":\"gray\",\"ticker\":null},\"id\":\"8196\",\"type\":\"Grid\"},{\"attributes\":{\"factors\":[\"wt_100_di_r1\",\"wt_100_di_r2\",\"wt_100_mono_r1\",\"wt_100_mono_r2\"],\"palette\":[\"#2b83ba\",\"#abdda4\",\"#fdae61\",\"#d7191c\"]},\"id\":\"8202\",\"type\":\"CategoricalColorMapper\"}],\"root_ids\":[\"8183\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.1\"}};\n  var render_items = [{\"docid\":\"ade1220c-643f-4b2e-ae03-2511d877dd06\",\"notebook_comms_target\":\"8244\",\"root_ids\":[\"8183\"],\"roots\":{\"8183\":\"1a434666-084d-4a6d-ba98-390efe12d259\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    var attempts = 0;\n    var timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "8183"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline finished successfully!\n",
      "Run time: 13.25 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "kill_pipeline = False\n",
    "samples_done = 0\n",
    "sample_runs = {}\n",
    "thread_tracker = OrderedDict()\n",
    "\n",
    "#start threads for running each sample through the pipeline\n",
    "for filename in fastq_files:\n",
    "    sample_runs[filename] = Thread(target=run_one_sample, args=(filename,))\n",
    "    sample_runs[filename].start()\n",
    "    \n",
    "print(\"Pipeline Running...\")\n",
    "\n",
    "steps_mapper = OrderedDict([ #offset by one because if it's recorded in the log, it's already done\n",
    "        \n",
    "        (\"initiate\", \"Deduplicating\"), \n",
    "        (\"deduplicate\", \"Trimming Adapter\"),\n",
    "        (\"trim\", \"Aligning to ncRNA\"),\n",
    "        (\"align_ncRNA\", \"Aligning to Genome\"),\n",
    "        (\"align_genome\", \"Sorting reporter aligned reads\"), \n",
    "        (\"sort_genome_aligned\", \"Indexing Genome Aligned Reads\"), \n",
    "        (\"index_genome_aligned\", \"Done\"),\n",
    "    ])\n",
    "\n",
    "steps = [\"Start\"]+[steps_mapper[step] for step in steps_mapper]\n",
    "prefixes = list(thread_tracker.keys())\n",
    "\n",
    "p = figure(height=200, width=800, y_range=prefixes, background_fill_color=\"lightgray\", x_range=steps, title=\"Pipeline Progress\", tools=[])\n",
    "p.xgrid.grid_line_color = \"gray\"\n",
    "p.xaxis.major_label_orientation = np.pi/8\n",
    "\n",
    "try: \n",
    "    pal = brewer[\"Spectral\"][len(prefixes)]\n",
    "except KeyError:\n",
    "    pal = [\"grey\"]*len(prefixes)\n",
    "\n",
    "p.ygrid.visible = False\n",
    "source = ColumnDataSource(data={\"y\":prefixes, \"right\":[\"Start\"]*len(prefixes), \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)})\n",
    "x = p.hbar(y=\"y\", right=\"right\", height=\"height\", left=\"left\", color=factor_cmap(field_name=\"y\", palette=pal, factors=prefixes), \n",
    "           line_color=\"black\", source=source)\n",
    "\n",
    "show(p, notebook_handle=True)\n",
    "\n",
    "#Check every 1 sec whether the pipeline is finished and report wether it's been terminated.\n",
    "try:\n",
    "    while samples_done != len(fastq_files):\n",
    "        for sample in thread_tracker:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "        sleep(1)\n",
    "    else:\n",
    "        if kill_pipeline == False:\n",
    "            source.data = {\"y\":prefixes, \"right\":[steps_mapper[thread_tracker[sample]] for sample in thread_tracker], \"height\":[0.5]*len(prefixes), \"left\":[0]*len(prefixes)}\n",
    "            x.view = CDSView(source=x.data_source)\n",
    "            push_notebook()\n",
    "            print(\"Pipeline finished successfully!\")\n",
    "        else:\n",
    "            print(\"Run terminated. Check for errors\")\n",
    "except:\n",
    "    kill_pipeline = True  #allows KeyboardInterrupt to kill pipeline\n",
    "    print(\"Run terminated. Check for errors\")\n",
    "    raise\n",
    "\n",
    "    \n",
    "\n",
    "runtime = time() - start_time\n",
    "if runtime > 60:\n",
    "    mins = round(runtime/60, 2)\n",
    "    print(\"Run time:\", mins, \"minutes\")\n",
    "else:\n",
    "    secs = round(runtime, 2)\n",
    "    print(\"Run time:\", secs, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fate(sample_name):\n",
    "    global output_dir\n",
    "    with open(output_dir+\"logs/\"+sample_name+\"_stdout.txt\", \"r\") as log, \\\n",
    "    open(output_dir+\"ncRNA_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as ncRNA_log, \\\n",
    "    open(output_dir+\"genome_aligned/\"+sample_name+\"/\"+sample_name+\"_Log.final.out\", \"r\") as genome_log:\n",
    "        file = log.readlines()[::-1]\n",
    "        trim_end = file.index(\"Align to ncRNA\\n\")\n",
    "        trim_start = file.index(\"--------------------\\n\", trim_end)\n",
    "        for line in file[trim_end:trim_start]:\n",
    "            if line.startswith(\"Input:\"):\n",
    "                input_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "            if line.startswith(\"Result\"):\n",
    "                trimmed_reads = int(line.split(\"\\t\")[1].split(\" \")[0])\n",
    "        file = ncRNA_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                ncRNA_mapped_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                ncRNA_mapped_reads += int(line.split(\"|\")[1].strip())\n",
    "        file = genome_log.readlines()\n",
    "        for line in file:\n",
    "            if line.strip().startswith(\"Number of input reads\"):\n",
    "                genome_input_reads = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Uniquely mapped reads number\"):\n",
    "                genome_mapped = int(line.split(\"|\")[1].strip())\n",
    "            elif line.strip().startswith(\"Number of reads mapped to multiple loci\"):\n",
    "                genome_mapped += int(line.split(\"|\")[1].strip())\n",
    "        unmapped_reads = genome_input_reads - genome_mapped\n",
    "        print(\"Of library,\", str(round(trimmed_reads/input_reads*100, 2))+\"%\", \"survived trimming.\")\n",
    "        print(str(round(ncRNA_mapped_reads/trimmed_reads*100, 2))+\"%\", \"mapped to ncRNA.\")\n",
    "        print(str(round(genome_mapped/trimmed_reads*100, 2))+\"%\", \"mapped to the genome.\")\n",
    "        print(str(round(unmapped_reads/trimmed_reads*100, 2))+\"%\", \"remained unmapped.\")\n",
    "                \n",
    "for barcode in barcode_name_mapping:\n",
    "    print(barcode_name_mapping[barcode])\n",
    "    read_fate(barcode_name_mapping[barcode])\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
